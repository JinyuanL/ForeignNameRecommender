{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/synonym/extractor.py:7: DeprecationWarning: This project has been depricated. Please use FlashText https://github.com/vi3k6i5/flashtext instead.\n",
      "  \"Please use FlashText https://github.com/vi3k6i5/flashtext instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['san francisco', 'new york', 'new york']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # import module\n",
    "# from synonym.extractor import SynonymExtractor\n",
    "\n",
    "# # Create an object of SynonymExtractor\n",
    "# synonym_extractor = SynonymExtractor()\n",
    "\n",
    "# # add synonyms\n",
    "# synonym_names = ['NY', 'new-york', 'SF']\n",
    "# clean_names = ['new york', 'new york', 'san francisco']\n",
    "\n",
    "# for synonym_name, clean_name in zip(synonym_names, clean_names):\n",
    "#     synonym_extractor.add_to_synonym(synonym_name, clean_name)\n",
    "\n",
    "# synonyms_found = synonym_extractor.get_synonyms_from_sentence('I love SF and NY. new-york is the best.')\n",
    "\n",
    "# synonyms_found\n",
    "# # >> ['san francisco', 'new york', 'new york']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n",
      "0.7092972666062738\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download('punkt') # if necessary...\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "\n",
    "print(cosine_sim('a little bird', 'a little bird'))\n",
    "print(cosine_sim('a little bird', 'a little bird chirps'))\n",
    "print(cosine_sim('a little bird', 'a big dog barks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4494364165239821\n",
      "0.0\n",
      "['T', 'h', 'e', ' ', 'j', 'o', 'y', ' ', 'o', 'r', ' ', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's', ' ', 'o', 'f', ' ', 'Y', 'a', 'h', 'w', 'e', 'h']\n"
     ]
    }
   ],
   "source": [
    "enMeaning = \"The joy or happiness of Yahweh\"\n",
    "enMeaning2=\"My Father is Joyful ; A Variant of Abby\"\n",
    "chMeaning = \"Happy\"\n",
    "print(cosine_sim(chMeaning,enMeaning))\n",
    "print(cosine_sim(chMeaning,enMeaning2))\n",
    "\n",
    "print(stem_tokens(\"The joy or happiness of Yahweh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
