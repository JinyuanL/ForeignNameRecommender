{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "version0: ['My', 'Father', 'is', 'Joyful', ';', 'A', 'Variant', 'of', 'Abby']\n",
      "\n",
      "\n",
      "version1: ['My', 'Father', 'is', 'Joyful', 'Variant', 'of', 'Abby']\n",
      "\n",
      "\n",
      "version2: ['My', 'Father', 'Joyful', 'Variant', 'Abby']\n",
      "\n",
      "\n",
      "version3: ['My', 'Father', 'Joy', 'Variant', 'Abbi']\n"
     ]
    }
   ],
   "source": [
    "#import the MeTA python bindings\n",
    "import metapy\n",
    "#If you'd like, you can tell MeTA to log to stderr so you can get progress output \n",
    "#when running long-running function calls.\n",
    "metapy.log_to_stderr()\n",
    "\n",
    "\n",
    "doc = metapy.index.Document()\n",
    "# doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "doc.content(\"My Father is Joyful ; A Variant of Abby\")\n",
    "\n",
    "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "tok.set_content(doc.content()) # this could be any string\n",
    "tokens = [token for token in tok]\n",
    "print(\"\\n\\nversion0:\",tokens)\n",
    "\n",
    "\n",
    "tok = metapy.analyzers.LengthFilter(tok, min=2, max=30)\n",
    "tok.set_content(doc.content()) # this could be any string\n",
    "tokens = [token for token in tok]\n",
    "print(\"\\n\\nversion1:\",tokens)\n",
    "\n",
    "\n",
    "tok = metapy.analyzers.ListFilter(tok, \"lemur-stopwords.txt\", metapy.analyzers.ListFilter.Type.Reject)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(\"\\n\\nversion2:\",tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tok = metapy.analyzers.Porter2Filter(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(\"\\n\\nversion3:\",tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Father Joy Variant Abbi \n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "for t in tokens:\n",
    "    s += (t+\" \")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n",
      "0.7092972666062738\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download('punkt') # if necessary...\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4494364165239821\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "enMeaning = \"The joy or happiness of Yahweh\"\n",
    "enMeaning2=\"My Father Joy Variant Abbi \"\n",
    "chMeaning = \"Happy\"\n",
    "print(cosine_sim(chMeaning,enMeaning))\n",
    "print(cosine_sim(chMeaning,enMeaning2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "My 0.0\n",
      "Father 0.0\n",
      "Joy 0.0\n",
      "Variant 0.0\n",
      "Abbi 0.0\n"
     ]
    }
   ],
   "source": [
    "finalWords = []\n",
    "for t in tokens:\n",
    "    finalWords.append(t)\n",
    "print(type(finalWords))\n",
    "print(type(finalWords[1]))\n",
    "\n",
    "for word in finalWords:\n",
    "    print(word, cosine_sim(\"Happy\",word))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
